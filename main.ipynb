{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPGGkHV2KhNeSG8PtoHOV0R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lukas4319/Animal_classification/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFdJ8ijLEl99"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_block_list, num_classes = 10, zero_init_residual = True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.stage1 = self.make_stage(block, 64, num_block_list[0], stride=1)\n",
        "        self.stage2 = self.make_stage(block, 128, num_block_list[1], stride=2)\n",
        "        self.stage3 = self.make_stage(block, 256, num_block_list[2], stride=2)\n",
        "        self.stage4 = self.make_stage(block, 512, num_block_list[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Sequential(nn.Linear(512 * block.expansion, 1024),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(1024, 512),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(512, 256),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(256, 128),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(128, 64),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(64, 32),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(32, 16),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(16, num_classes))\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, block):\n",
        "                    nn.init.constant_(m.residual[-1].weight, 0)\n",
        "\n",
        "    def make_stage(self, block, inner_channels, num_blocks, stride = 1):\n",
        "\n",
        "        if stride != 1 or self.in_channels != inner_channels * block.expansion:\n",
        "            projection = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, inner_channels * block.expansion, 1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(inner_channels * block.expansion))\n",
        "        else:\n",
        "            projection = None\n",
        "\n",
        "        layers = []\n",
        "        layers += [block(self.in_channels, inner_channels, stride, projection)]\n",
        "        self.in_channels = inner_channels * block.expansion\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers += [block(self.in_channels, inner_channels)]\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.stage1(x)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.stage4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet18(**kwargs):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "\n",
        "def resnet34(**kwargs):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "\n",
        "def resnet50(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)"
      ],
      "metadata": {
        "id": "KQONMUxEFWJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exec(f\"model = {model_type}().to(DEVICE)\")\n",
        "print(model)\n",
        "x_batch, _ = next(iter(train_DL))\n",
        "print(model(x_batch.to(DEVICE)).shape)"
      ],
      "metadata": {
        "id": "bfvITcXvFXhg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}