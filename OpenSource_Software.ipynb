{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPOBQQWu85t+iiJY2T2QYWm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lukas4319/Animal_classification/blob/main/OpenSource_Software.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "EZ3ab70yyqqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 라이브러리 호출 및 GPU 설정"
      ],
      "metadata": {
        "id": "sP5H2IhgQfho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks\")\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn, optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import wandb\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "HsplN8K9Qf39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Hyper Parameter 및 경로 설정"
      ],
      "metadata": {
        "id": "D_GPll8GQX-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "LR = 0.0001\n",
        "LR_STEP = 5\n",
        "LR_GAMMA = 0.9\n",
        "EPOCH = 20\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "new_model_train = True\n",
        "model_type = \"resnet18\"\n",
        "dataset = \"Animal10\"\n",
        "save_model_path = f\"/content/drive/MyDrive/Colab Notebooks/result/{model_type}_{dataset}.pt\"\n",
        "save_history_path = f\"/content/drive/MyDrive/Colab Notebooks/result/{model_type}_history_{dataset}.pt\""
      ],
      "metadata": {
        "id": "a7TtdDBRQXhZ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Load"
      ],
      "metadata": {
        "id": "y3vU60B7w2Yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown==4.6.0"
      ],
      "metadata": {
        "id": "b0WTHwtEw4Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=#data_id"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BBf2BvFUw_4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Animals10.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a9sDklJ8xFra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ],
      "metadata": {
        "id": "a3zJbBQtREqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                      transforms.ToTensor()])\n",
        "transform_val = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                      transforms.ToTensor()])\n",
        "transform_test = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                      transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "bMCcXMIuQXxb"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_DS = torchvision.datasets.ImageFolder(root = \"/content/Animals10/train_DS\", transform = transform_train)\n",
        "val_DS = torchvision.datasets.ImageFolder(root = \"/content/Animals10/val_DS\", transform = transform_val)\n",
        "test_DS = torchvision.datasets.ImageFolder(root = \"/content/Animals10/test_DS\", transform = transform_test)"
      ],
      "metadata": {
        "id": "iQyZ83gERJav"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_DL = DataLoader(train_DS, batch_size = BATCH_SIZE, shuffle = True)\n",
        "val_DL = DataLoader(val_DS, batch_size = BATCH_SIZE, shuffle = True)\n",
        "test_DL = DataLoader(test_DS, batch_size = BATCH_SIZE, shuffle = True)"
      ],
      "metadata": {
        "id": "EDHJWxl5RKvx"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train & Test"
      ],
      "metadata": {
        "id": "NN2eiwpaRRsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, train_loader, val_loader, criterion, optimizer, device=DEVICE):\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.scheduler = None\n",
        "        self.history = {\n",
        "            \"train_loss\": [],\n",
        "            \"val_loss\": [],\n",
        "            \"train_acc\": [],\n",
        "            \"val_acc\": [],\n",
        "        }\n",
        "        self.best_loss = float(\"inf\")\n",
        "\n",
        "    def set_scheduler(self, step_size, gamma=0.1):\n",
        "        self.scheduler = StepLR(self.optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "    def _run_epoch(self, loader, is_train=True):\n",
        "        mode = \"train\" if is_train else \"val\"\n",
        "        self.model.train() if is_train else self.model.eval()\n",
        "\n",
        "        running_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for x_batch, y_batch in tqdm(loader, desc=f\"{mode} Epoch\", leave=False):\n",
        "            x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
        "            with torch.set_grad_enabled(is_train):\n",
        "                y_pred = self.model(x_batch)\n",
        "                loss = self.criterion(y_pred, y_batch)\n",
        "\n",
        "                if is_train:\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * x_batch.size(0)\n",
        "            correct += (y_pred.argmax(1) == y_batch).sum().item()\n",
        "            total += x_batch.size(0)\n",
        "\n",
        "        avg_loss = running_loss / total\n",
        "        accuracy = correct / total * 100\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    def train(self, epochs, save_model_path, log_wandb=False):\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            current_lr = self.optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, LR: {current_lr}\")\n",
        "\n",
        "            train_loss, train_acc = self._run_epoch(self.train_loader, is_train=True)\n",
        "            val_loss, val_acc = self._run_epoch(self.val_loader, is_train=False)\n",
        "\n",
        "            self.history[\"train_loss\"].append(train_loss)\n",
        "            self.history[\"val_loss\"].append(val_loss)\n",
        "            self.history[\"train_acc\"].append(train_acc)\n",
        "            self.history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "            if val_loss < self.best_loss:\n",
        "                self.best_loss = val_loss\n",
        "                torch.save(self.model.state_dict(), save_model_path)\n",
        "\n",
        "            if self.scheduler:\n",
        "                self.scheduler.step()\n",
        "\n",
        "            epoch_time = time.time() - start_time\n",
        "            print(\n",
        "                f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
        "                f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%, Time: {epoch_time:.2f}s\"\n",
        "            )\n",
        "\n",
        "            if log_wandb:\n",
        "                wandb.log({\n",
        "                    \"train_loss\": train_loss,\n",
        "                    \"val_loss\": val_loss,\n",
        "                    \"train_acc\": train_acc,\n",
        "                    \"val_acc\": val_acc,\n",
        "                    \"epoch\": epoch,\n",
        "                })\n",
        "\n",
        "        return self.history\n",
        "\n",
        "    def evaluate(self, test_loader):\n",
        "        test_loss, test_acc = self._run_epoch(test_loader, is_train=False)\n",
        "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
        "        return test_loss, test_acc"
      ],
      "metadata": {
        "id": "cM-Kd9NBRQCQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 생성"
      ],
      "metadata": {
        "id": "h9meJjsERiPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, inner_channels, stride = 1, projection = None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.residual = nn.Sequential(nn.Conv2d(in_channels, inner_channels, 3, stride=stride, padding=1, bias=False),\n",
        "                                      nn.BatchNorm2d(inner_channels),\n",
        "                                      nn.ReLU(inplace=True),\n",
        "                                      nn.Conv2d(inner_channels, inner_channels * self.expansion, 3, padding=1, bias=False),\n",
        "                                      nn.BatchNorm2d(inner_channels))\n",
        "        self.projection = projection\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        residual = self.residual(x)\n",
        "\n",
        "        if self.projection is not None:\n",
        "            shortcut = self.projection(x) # 점선 연결\n",
        "        else:\n",
        "            shortcut = x # 실선 연결\n",
        "\n",
        "        out = self.relu(residual + shortcut)\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, inner_channels, stride = 1, projection = None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.residual = nn.Sequential(nn.Conv2d(in_channels, inner_channels, 1, bias=False),\n",
        "                                      nn.BatchNorm2d(inner_channels),\n",
        "                                      nn.ReLU(inplace=True),\n",
        "                                      nn.Conv2d(inner_channels, inner_channels, 3, stride=stride, padding=1, bias=False),\n",
        "                                      nn.BatchNorm2d(inner_channels),\n",
        "                                      nn.ReLU(inplace=True),\n",
        "                                      nn.Conv2d(inner_channels, inner_channels * self.expansion, 1, bias=False),\n",
        "                                      nn.BatchNorm2d(inner_channels * self.expansion))\n",
        "\n",
        "        self.projection = projection\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.residual(x)\n",
        "\n",
        "        if self.projection is not None:\n",
        "            shortcut = self.projection(x)\n",
        "        else:\n",
        "            shortcut = x\n",
        "\n",
        "        out = self.relu(residual + shortcut)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_block_list, num_classes = 10, zero_init_residual = True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.stage1 = self.make_stage(block, 64, num_block_list[0], stride=1)\n",
        "        self.stage2 = self.make_stage(block, 128, num_block_list[1], stride=2)\n",
        "        self.stage3 = self.make_stage(block, 256, num_block_list[2], stride=2)\n",
        "        self.stage4 = self.make_stage(block, 512, num_block_list[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Sequential(nn.Linear(512 * block.expansion, 1024),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(1024, 512),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(512, 256),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(256, 128),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(128, 64),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(64, 32),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(32, 16),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(16, num_classes))\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, block):\n",
        "                    nn.init.constant_(m.residual[-1].weight, 0)\n",
        "\n",
        "    def make_stage(self, block, inner_channels, num_blocks, stride = 1):\n",
        "\n",
        "        if stride != 1 or self.in_channels != inner_channels * block.expansion:\n",
        "            projection = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, inner_channels * block.expansion, 1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(inner_channels * block.expansion))\n",
        "        else:\n",
        "            projection = None\n",
        "\n",
        "        layers = []\n",
        "        layers += [block(self.in_channels, inner_channels, stride, projection)]\n",
        "        self.in_channels = inner_channels * block.expansion\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers += [block(self.in_channels, inner_channels)]\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.stage1(x)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.stage4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "sR_1EBfaRlJ6"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet18(**kwargs):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "\n",
        "def resnet34(**kwargs):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "\n",
        "def resnet50(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)"
      ],
      "metadata": {
        "id": "Pm0e7qDaRngp"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exec(f\"model = {model_type}().to(DEVICE)\")\n",
        "print(model)\n",
        "x_batch, _ = next(iter(train_DL))\n",
        "print(model(x_batch.to(DEVICE)).shape)"
      ],
      "metadata": {
        "id": "fwvzkTQKSC7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Train"
      ],
      "metadata": {
        "id": "qVNJShJTwQm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if new_model_train:\n",
        "    optimizer = optim.AdamW(model.parameters(), lr = LR)\n",
        "    trainer = Trainer(model=model,\n",
        "                      train_loader=train_DL,\n",
        "                      val_loader=val_DL,\n",
        "                      criterion=criterion,\n",
        "                      optimizer=optimizer,\n",
        "                      device=DEVICE)\n",
        "    trainer.set_scheduler(step_size=LR_STEP, gamma=LR_GAMMA)\n",
        "    history = trainer.train(epochs=EPOCH,\n",
        "                            save_model_path=save_model_path,\n",
        "                            log_wandb=False)\n",
        "elif new_model_train:\n",
        "    optimizer = optim.AdamW(model.parameters(), lr = LR)\n",
        "    trainer = Trainer(model=load_model,\n",
        "                      train_loader=train_DL,\n",
        "                      val_loader=val_DL,\n",
        "                      criterion=criterion,\n",
        "                      optimizer=optimizer,\n",
        "                      device=DEVICE)\n",
        "    trainer.set_scheduler(step_size=LR_STEP, gamma=LR_GAMMA)\n",
        "    history = trainer.train(epochs=EPOCH,\n",
        "                            save_model_path=save_model_path,\n",
        "                            log_wandb=False)"
      ],
      "metadata": {
        "id": "Z7yh2ksRSG3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Load"
      ],
      "metadata": {
        "id": "PIjv3DQiwTqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_model = model.load_state_dict(torch.load(save_model_path))"
      ],
      "metadata": {
        "id": "q3nwi0VfwTOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Test"
      ],
      "metadata": {
        "id": "hfPF1cIiwnjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = trainer.evaluate(test_DL)"
      ],
      "metadata": {
        "id": "BJ-p35siwnPM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
